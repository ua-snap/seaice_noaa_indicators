{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44b9eed-a8ff-411a-a03b-9182727ca367",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "This notebook runs the pipeline which creates the sea ice indicators dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb91bdf-3420-443e-8472-bb4c3167561a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0: Setup\n",
    "\n",
    "Run the cell below before running any other cell in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401909c8-c031-4d51-805f-1f1c87f2c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User parameters\n",
    "# Set begin and end years for running the pipeline\n",
    "begin_year = 1978\n",
    "end_year = 2019\n",
    "\n",
    "# setup environment and handle all file-pathing\n",
    "# Allow printing some things to the terminal\n",
    "terminal_output = open(\"/dev/stdout\", \"w\")\n",
    "print(\"Executing pipeline Step 0 (setup)\", file=terminal_output, flush=True)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.getenv(\"BASE_DIR\"))\n",
    "output_dir = Path(os.getenv(\"OUTPUT_DIR\"))\n",
    "scratch_dir = Path(os.getenv(\"SCRATCH_DIR\"))\n",
    "\n",
    "# raw NSIDC-0051 data dir\n",
    "raw_0051_dir = base_dir.joinpath(\"nsidc_0051/raw\")\n",
    "raw_0051_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# prepped NSIDC-0051 data dir\n",
    "prepped_0051_dir = base_dir.joinpath(\"nsidc_0051/prepped\")\n",
    "prepped_0051_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# smoothed daily time series data\n",
    "smoothed_0051_fp = base_dir.joinpath(f\"nsidc_0051/nsidc_0051_sic_{begin_year}-{end_year}_smoothed.nc\")\n",
    "\n",
    "# final output indicators\n",
    "# final dataset starts in 1979\n",
    "fubu_fp = output_dir.joinpath(f\"arctic_seaice_fubu_dates_{begin_year + 1}-{end_year}.nc\")\n",
    "\n",
    "print(\"Pipeline Step 0 (setup) complete\\n\", file=terminal_output, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f29dd6-5d38-41f8-9f59-9d2cb8d10857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1: Download NSIDC-0051 daily data\n",
    "\n",
    "Download the NSIDC-0051 passive microwave sea ice concentration data from NSIDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14df10ae-4081-424b-8dfc-20df17cea3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=NSIDC-0051&version=001&version=01&version=1&temporal[]=1978-10-26T00:00:00Z,2019-12-31T23:59:59Z&producer_granule_id[]=*&options[producer_granule_id][pattern]=true\n",
      "\n",
      "Found 27770 matches.\n",
      "..."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/pipeline/download_nsidc.py\u001b[0m in \u001b[0;36mcmr_search\u001b[0;34m(short_name, version, time_start, time_end, polygon, filename_filter)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cmr-scroll-id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmr_scroll_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcmr_scroll_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1360\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/envs/default/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_444/3411607823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtime_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpolygon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mfilename_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/UA/kmredilla/seaice_noaa_indicators/pipeline/download_nsidc.py\u001b[0m in \u001b[0;36mcmr_search\u001b[0;34m(short_name, version, time_start, time_end, polygon, filename_filter)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quit' is not defined"
     ]
    }
   ],
   "source": [
    "# User parameters\n",
    "# clobber - set to True to always re-download all files, \n",
    "#   regardless of presence in out_0051_dir. \n",
    "# Default is False, download only files already found.\n",
    "clobber = False\n",
    "\n",
    "print(\"Executing pipeline Step 1 (download NSIDC-0051)\", file=terminal_output, flush=True)\n",
    "\n",
    "import download_nsidc as dsic\n",
    "\n",
    "version = \"1\"\n",
    "time_start = f\"{begin_year}-10-26T00:00:00Z\"\n",
    "time_end = f\"{end_year}-12-31T23:59:59Z\"\n",
    "polygon = \"\"\n",
    "filename_filter = \"*\"\n",
    "\n",
    "dsic.cmr_url = \"https://cmr.earthdata.nasa.gov\"\n",
    "dsic.urs_url = \"https://urs.earthdata.nasa.gov\"\n",
    "dsic.cmr_page_size = 2000\n",
    "dsic.cmr_file_url = (\n",
    "    \"{0}/search/granules.json?provider=NSIDC_ECS\"\n",
    "    \"&sort_key[]=start_date&sort_key[]=producer_granule_id\"\n",
    "    \"&scroll=true&page_size={1}\".format(dsic.cmr_url, dsic.cmr_page_size)\n",
    ")\n",
    "\n",
    "urls = dsic.cmr_search(\n",
    "    \"NSIDC-0051\",\n",
    "    version,\n",
    "    time_start,\n",
    "    time_end,\n",
    "    polygon=polygon,\n",
    "    filename_filter=filename_filter,\n",
    ")\n",
    "\n",
    "# dunno what these new files are yet, but remove them\n",
    "urls = [url for url in urls if \"s.bin\" not in Path(url).name]\n",
    "# also filter out monthly data urls\n",
    "urls = [url for url in urls if len(Path(url).name.split(\"_\")[1]) == 8]  \n",
    "\n",
    "if not clobber:\n",
    "    # filter out already downloaded files\n",
    "    local_fps = [fp.name for fp in list(raw_0051_dir.glob(\"*\"))]\n",
    "    urls = [url for url in urls if Path(url).name not in local_fps]\n",
    "    print(\n",
    "        f\"Not overwriting existing NSIDC-0051 files. {len(urls)} files will be downloaded\", \n",
    "        file=terminal_output, \n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "if len(urls) > 0:\n",
    "    results_di = dsic.cmr_download(urls, raw_0051_dir)\n",
    "    print(f\"NSIDC-0051 data written to {raw_0051_dir}\", file=terminal_output, flush=True)\n",
    "    \n",
    "print(\"Pipeline Step 1 (download NSIDC-0051) complete\\n\", file=terminal_output, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb478206-1f1f-4f6d-b088-30a985a70a46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2: Convert NSIDC-0051 binary data files to GeoTIFF\n",
    "\n",
    "Convert the raw binary files downloaded from NSIDC to GeoTIFFs for assembling a complete daily timeseries object. Writes converted GeoTIFFs to `$BASE_DIR/nsidc_0051/prepped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1682f4e9-ca58-4e07-a508-7547e5c15151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not overwriting existing converted files. 0 files will be converted\n"
     ]
    }
   ],
   "source": [
    "# User parameters\n",
    "# set clobber to True to download even if NSIDC-0747 \n",
    "#   dataset exists locally.\n",
    "clobber = False\n",
    "# set the number of CPUs to use for converting the \n",
    "ncpus = 32\n",
    "\n",
    "print(\"Executing pipeline Step 2 (convert binary data to GeoTIFF)\", file=terminal_output, flush=True)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from convert_nsidc import convert_bin_to_gtiff\n",
    "\n",
    "\n",
    "# list filepaths from input dir\n",
    "fps = [fp for fp in raw_0051_dir.glob(\"*.bin\")]\n",
    "\n",
    "# if clobber not specifed, check for existing files in out_dir\n",
    "if not clobber:\n",
    "    converted_fns = [fp.name.replace(\"v1-1\", \"v1.1\").split(\".tif\")[0] for fp in prepped_0051_dir.glob(\"*\")]\n",
    "    fps = [fp for fp in fps if fp.name.split(\".bin\")[0] not in converted_fns]\n",
    "    print(\n",
    "        f\"Not overwriting existing converted files. {len(fps)} files will be converted\", \n",
    "        file=terminal_output, \n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "if len(fps) > 0:\n",
    "    print(f\"Converting {len(fps)} files\", file=terminal_output, end=\"...\", flush=True)\n",
    "    tic = time.perf_counter()\n",
    "    args = [(fp, prepped_0051_dir) for fp in fps]\n",
    "    with Pool(ncpus) as pool:\n",
    "        out = pool.starmap(convert_bin_to_gtiff, args)\n",
    "    print(f\"done, {round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "    \n",
    "print(\"Pipeline Step 2 (convert binary data to GeoTIFF) complete\", file=terminal_output, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81387b6e-4b84-43fb-a379-c506b4bd68f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3: Make smoothed daily time series\n",
    "\n",
    "Create a complete time series object of statistically smoothed daily SIC observations in preparation for the FUBU thresholding algorithm. Writes the smoothed output to `$BASE_DIR/nsidc_0051/nsidc_0051_sic_<begin year>-<end year>_smoothed.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c417bc4d-d41b-4a94-b476-1c557a3256a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack the irregularly spaced data to an xarray.DataSet...done, 36.6s\n",
      "Interpolate to daily values...done, 37.6s\n",
      "Perform spatial smooth...done, 95.6s\n",
      "Perform three rounds of Hanning smoothing...pass 1 complete...pass 2 complete...pass 3 complete...done, 103.0s\n",
      "Smoothed dataset written to /workspace/Shared/Tech_Projects/SeaIce_NOAA_Indicators/project_data/nsidc_0051/nsidc_0051_sic_1978-2019_smoothed.nc\n"
     ]
    }
   ],
   "source": [
    "# User parameters\n",
    "# set the number of CPUs to use for creating the daily timeseries.\n",
    "ncpus = 32\n",
    "\n",
    "print(\"Executing pipeline Step 3 (make smoothed daily time series)\", file=terminal_output, flush=True)\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# standard\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "# non-standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "# project\n",
    "import make_daily_timeseries as mds\n",
    "\n",
    "\n",
    "fps = sorted(list(prepped_0051_dir.glob(\"*\")))\n",
    "data_times = [mds.make_datetimes(fp.name.split(\".\")[0].split(\"_\")[1]) for fp in fps]\n",
    "\n",
    "# stack the irregularly spaced data to a netcdf\n",
    "print(\"Stack the irregularly spaced data to an xarray.DataSet\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "with rio.open(fps[0]) as template:\n",
    "    meta = template.meta.copy()\n",
    "    height, width = template.shape\n",
    "\n",
    "arr = mds.stack_rasters(fps, ncpus=ncpus)\n",
    "ds = mds.make_xarray_dset(arr.copy(), pd.DatetimeIndex(data_times), meta)\n",
    "da = ds[\"sic\"].copy()\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "# interpolate to daily\n",
    "print(\"Interpolate to daily values\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "da_interp = da.resample(time=\"1D\").asfreq()\n",
    "\n",
    "# get a masks layer from the raw files.  These are all values > 250\n",
    "# ------------ ------------ ------------ ------------ ------------ ------------ ------------\n",
    "# 251 Circular mask used in the Arctic to cover the irregularly-shaped data\n",
    "#       gap around the pole (caused by the orbit inclination and instrument swath)\n",
    "# 252 Unused\n",
    "# 253 Coastlines\n",
    "# 254 Superimposed land mask\n",
    "# 255 Missing data\n",
    "# make a mask of the known nodata values when we start...\n",
    "mask = (arr[0] > 250) & (arr[0] < 300)\n",
    "\n",
    "# set masks to nodata\n",
    "dat = da_interp.values.copy()\n",
    "\n",
    "# make the nodata mask np.nan for computations\n",
    "out_masked = []\n",
    "for i in dat:\n",
    "    i[mask] = np.nan\n",
    "    out_masked = out_masked + [i]\n",
    "\n",
    "# put the cleaned up data back into the stacked NetCDF\n",
    "da_interp.data = np.array(out_masked)\n",
    "da_interp.data = np.apply_along_axis(mds.interpolate, axis=0, arr=da_interp).round(4)\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "# spatially smooth the 2-D daily slices of data using a \n",
    "#   mean generic filter (without any aggregation)\n",
    "print(f\"Perform spatial smooth\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "footprint = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "\n",
    "# 'run using multiprocessing -- YMMV this is a tad flaky at times.'\n",
    "# ^^ comment from original author, not sure where flakiness manifested\n",
    "# previously but left this note anyway.\n",
    "\n",
    "# Some profiling revealed that paritioning the interpolated array\n",
    "#   and looping over it with 10 cores on the dev machine (Atlas) was \n",
    "#   about optimal.\n",
    "filter_ncpus = 10\n",
    "filter_slices = mds.chunkit(da_interp.values.shape[0], filter_ncpus)\n",
    "args = [(da_interp.values[sl], footprint) for sl in filter_slices]\n",
    "with Pool(filter_ncpus) as pool:\n",
    "    out = pool.starmap(mds.run_jit_mean_filter, args)\n",
    "\n",
    "# mask the spatial smoothed outputs with the mask at each 2D slice.\n",
    "smoothed = np.array([mds._maskit(i, mask) for i in np.concatenate(out)]).copy()\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "print(\"Perform three rounds of Hanning smoothing\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "n = 3  # perform 3 iterative smooths on the same series\n",
    "for i in range(n):\n",
    "    smoothed = np.apply_along_axis(mds.hanning_smooth, arr=smoothed, axis=0)\n",
    "    print(f\"pass {i + 1} complete\", end=\"...\", file=terminal_output)\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "print(\"Clean up data and re-mask\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# make sure no values < 0, set to 0\n",
    "smoothed[np.where((smoothed < 0) & (~np.isnan(smoothed)))] = 0\n",
    "\n",
    "# make sure no values > 1, set to 1\n",
    "smoothed[np.where((smoothed > 1) & (~np.isnan(smoothed)))] = 1\n",
    "\n",
    "# mask it again to make sure the nodata and land are properly masked following hanning.\n",
    "smoothed = np.array([mds._maskit(i, mask) for i in smoothed]).copy()\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", flush=True)\n",
    "\n",
    "print(\"Write smoothed daily dataset to a single netCDF\", end=\"...\", file=terminal_output, flush=True)\n",
    "# write it out as a NetCDF\n",
    "out_ds = da_interp.copy(deep=True)\n",
    "out_ds.values = smoothed.astype(np.float32)\n",
    "out_ds = out_ds.to_dataset(name=\"sic\")\n",
    "out_ds.attrs = ds.attrs\n",
    "# output encoding\n",
    "encoding = out_ds.sic.encoding.copy()\n",
    "encoding.update({\"zlib\": True, \"comp\": 5, \"contiguous\": False, \"dtype\": \"float32\"})\n",
    "out_ds.sic.encoding = encoding\n",
    "\n",
    "out_ds.to_netcdf(smoothed_0051_fp, format=\"NETCDF4\")\n",
    "print(\n",
    "    f\"done, {np.round(time.perf_counter() - tic)}s. Smoothed daily SIC time series written to {smoothed_0051_fp}\", \n",
    "    file=terminal_output, \n",
    "    flush=True\n",
    ")\n",
    "print(\n",
    "    (\"Pipeline Step 3 (make smoothed daily timeseries) completed at \"\n",
    "     f\"{dt.datetime.utcfromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "     \", elapsed time: {round(time.perf_counter() - start_time) / 60}m\\n\"),\n",
    "    file=terminal_output,\n",
    "    flush=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270d8eb-f7ea-42cd-b62e-85b29c3c2bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Compute Freeze-up / Break-up dates\n",
    "\n",
    "Compute the freeze-up / break-up start/end dates from the smoothed SIC daily time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e62936-4685-4318-bea1-6ab294cff369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting indicators program.\n",
      "Load smoothed SIC daily time series data...done, 45.0s\n",
      "Get statistics for FUBU thresholding algorithm...done, 53.0s\n",
      "Running the FUBU thresholding algorithm...1979 complete. 1980 complete. 1981 complete. 1982 complete. 1983 complete. 1984 complete. 1985 complete. 1986 complete. 1987 complete. 1988 complete. 1989 complete. 1990 complete. 1991 complete. 1992 complete. 1993 complete. 1994 complete. 1995 complete. 1996 complete. 1997 complete. 1998 complete. 1999 complete. 2000 complete. 2001 complete. 2002 complete. 2003 complete. 2004 complete. 2005 complete. 2006 complete. 2007 complete. 2008 complete. 2009 complete. 2010 complete. 2011 complete. 2012 complete. 2013 complete. 2014 complete. 2015 complete. 2016 complete. 2017 complete. 2018 complete. done, 467.0s\n",
      "Make xarray.DataSet from yearly results and write as CF-compliant netCDF to $OUTPUT_DIR...done, 468.0s. Indicators dataset written to /workspace/Shared/Tech_Projects/SeaIce_NOAA_Indicators/final_products/arctic_seaice_fubu_dates_1979-2019.nc.\n",
      "Indicators program completed at 2021-09-21 18:01:05, elapsed time: 8m\n"
     ]
    }
   ],
   "source": [
    "# User parameters\n",
    "# set the number of CPUs to use for creating the daily timeseries.\n",
    "ncpus = 32\n",
    "\n",
    "print(\"Executing pipeline Step 4 (compute freeze-up/break-up dates)\", file=terminal_output, flush=True)\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# standard\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "# non-standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pyproj.crs import CRS\n",
    "# project\n",
    "import compute_fubu as cpf\n",
    "\n",
    "\n",
    "# Open dataset of smoothed daily SIC\n",
    "# load it so it processes a LOT faster plus it is small...\n",
    "print(\"Load smoothed SIC daily time series data\", end=\"...\", file=terminal_output, flush=True)\n",
    "tic = time.perf_counter()\n",
    "ds = xr.load_dataset(smoothed_0051_fp)  \n",
    "\n",
    "# slice the data to the full years... currently this is 1979-20**\n",
    "ds_sic = ds.sel(time=slice(\"1979\", str(end_year)))[\"sic\"]\n",
    "years = (\n",
    "    ds_sic.time.to_index().map(lambda x: x.year).unique().tolist()[:-1]\n",
    ")  # cant compute last year\n",
    "\n",
    "# set all nodata pixels to np.nan\n",
    "ds_sic.data[ds_sic.data > 1] = np.nan\n",
    "\n",
    "# make a no data mask\n",
    "mask = np.isnan(ds_sic.isel(time=0).data)\n",
    "\n",
    "# get the summer and winter seasons that were determined in table 1 in the paper\n",
    "summer = ds_sic.sel(time=cpf.get_summer(ds_sic[\"time.month\"]))\n",
    "winter = ds_sic.sel(time=cpf.get_winter(ds_sic[\"time.month\"]))\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "# get the means and standard deviations of these season aggregates\n",
    "print(\"Get statistics for FUBU thresholding algorithm\", end=\"...\", file=terminal_output, flush=True)\n",
    "\n",
    "summer_mean = summer.groupby(\"time.year\").mean(dim=\"time\").round(4)\n",
    "summer_std = summer.groupby(\"time.year\").std(dim=\"time\", ddof=1).round(4)\n",
    "winter_mean = winter.groupby(\"time.year\").mean(dim=\"time\").round(4)\n",
    "winter_std = winter.groupby(\"time.year\").std(dim=\"time\", ddof=1).round(4)\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output)\n",
    "\n",
    "print(\"Running the FUBU thresholding algorithm\", end=\"...\", file=terminal_output, flush=True)\n",
    "f = partial(\n",
    "    cpf.wrap_fubu,\n",
    "    ds_sic=ds_sic,\n",
    "    summer_mean=summer_mean,\n",
    "    summer_std=summer_std,\n",
    "    winter_mean=winter_mean,\n",
    "    winter_std=winter_std,\n",
    "    ncpus=ncpus,\n",
    ")\n",
    "\n",
    "# run serial\n",
    "fubu_years = {year: f(year) for year in years}\n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s\", file=terminal_output, flush=True)\n",
    "\n",
    "# Make xarray.DataSet from yearly results, stack into arrays by indicator type\n",
    "print(\"Make xarray.DataSet from yearly results and write as CF-compliant netCDF to $OUTPUT_DIR\", end=\"...\", file=terminal_output, flush=True)\n",
    "stacked = {\n",
    "    indicator: np.array([fubu_years[year][indicator] for year in years])\n",
    "    for indicator in fubu_years[list(fubu_years.keys())[0]].keys()\n",
    "}\n",
    "fubu = cpf.make_cf_dataset(stacked, years, ds.coords)\n",
    "\n",
    "# dump to disk\n",
    "fubu.to_netcdf(fubu_fp)    \n",
    "print(f\"done, {np.round(time.perf_counter() - tic)}s. Indicators dataset written to {fubu_fp}.\", file=terminal_output, flush=True)\n",
    "print(\n",
    "    (\"Pipeline Step 4 (compute freeze-up/break-up dates) completed at \"\n",
    "     f\"{datetime.utcfromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "     f\", elapsed time: {round((time.perf_counter() - start_time) / 60)}m\\n\"), \n",
    "    file=terminal_output,\n",
    "    flush=True\n",
    ")\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# SOME NOTES ABOUT TRANSLATING FROM MLAB :\n",
    "# ----------------------------------------\n",
    "# [ 1 ]: to make an ordinal date that matches the epoch used by matlab in python\n",
    "# ordinal_date = date.toordinal(date(1971,1,1)) + 366\n",
    "# if not the number will be 366 days off due to the epoch starting January 0, 0000 whereas in Py Jan 1, 0001.\n",
    "# [ 2 ]: when computing stdev it is important to set the ddof=1 which is the matlab default.  Python leaves it at 0 default.\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1c0b3-81ac-4bcf-9a38-658e56f96735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
